{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8dUDDo6HXhDFfmO1YI8dv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RadwaFathi/ImageProcessing-ComputerVision/blob/main/ImageProject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import needed libraries"
      ],
      "metadata": {
        "id": "KZ6Rk4Jisywk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg7Vp8eCe2GJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4099e6c7-38a1-4072-fc0b-77f281fbca8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#add imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive\n",
        "import gdown\n",
        "from moviepy.editor import ImageSequenceClip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Video Collection"
      ],
      "metadata": {
        "id": "D983bgVosWIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-W-XtGhe4hk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a1265a-4244-4814-93da-2ce713fe6093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FlF50_uqXuBCRx1Mu5FaCvxUwz5EqWEs\n",
            "To: /content/Computer_Vision_Car_Video.mp4\n",
            "100%|██████████| 3.03M/3.03M [00:00<00:00, 49.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video downloaded successfully as Computer_Vision_Car_Video.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download video from Google Drive\n",
        "shared_link = \"https://drive.google.com/file/d/1FlF50_uqXuBCRx1Mu5FaCvxUwz5EqWEs/view?usp=sharing\"\n",
        "file_id = shared_link.split(\"/d/\")[1].split(\"/view\")[0]\n",
        "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "video_path = \"Computer_Vision_Car_Video.mp4\"\n",
        "gdown.download(download_url, video_path, quiet=False)\n",
        "print(f\"Video downloaded successfully as {video_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Video Preprocessing"
      ],
      "metadata": {
        "id": "cyBzVGm_shRo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mG2gmzdfEBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0924a25-a12f-406d-b917-c805935fe807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 120 frames.\n"
          ]
        }
      ],
      "source": [
        "# Create directory for frames\n",
        "frames_dir = \"/content/frames\"\n",
        "os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        " # Extract at least 20 frames per second\n",
        "\n",
        "frame_interval = int(fps / 20)\n",
        "frame_count = 0\n",
        "# Store some selected frames for further processing\n",
        "selected_frames = []\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_count % frame_interval == 0:\n",
        "        #convert frames to gray scale\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        frame_filename = os.path.join(frames_dir, f\"frame_{frame_count}.jpg\")\n",
        "\n",
        "        #save frames to a directory\n",
        "        cv2.imwrite(frame_filename, gray_frame)\n",
        "        selected_frames.append(gray_frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "print(f\"Extracted {len(selected_frames)} frames.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Apply Image Processing Techniques"
      ],
      "metadata": {
        "id": "qDXnFPOrrlgr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIux9t2g71zO"
      },
      "source": [
        "Histogram Analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_fYzs0L7zA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7437f2-c0c3-4e04-c25a-8623576deeae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Histogram analysis applied to 120 frames.\n",
            "Histogram images saved in histogram_analysis directory\n"
          ]
        }
      ],
      "source": [
        "# Store histogram visualization frames\n",
        "histogram_frames = []\n",
        "histogram_output_dir = \"histogram_analysis\"\n",
        "os.makedirs(histogram_output_dir, exist_ok=True)\n",
        "\n",
        "for i, gray_frame in enumerate(selected_frames):\n",
        "    # Create histogram visualization\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.hist(gray_frame.ravel(), bins=256, range=[0,256], color='gray', alpha=0.7)\n",
        "    plt.title(f\"Histogram of Frame {i}\")\n",
        "    plt.xlabel(\"Pixel Intensity\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "\n",
        "    # Save the histogram plot to an image file\n",
        "    hist_filename = os.path.join(histogram_output_dir, f\"hist_{i:04d}.png\")\n",
        "    plt.savefig(hist_filename)\n",
        "    plt.close()\n",
        "\n",
        "    # Read back the saved image and convert to OpenCV format\n",
        "    hist_img = cv2.imread(hist_filename)\n",
        "    histogram_frames.append(hist_img)\n",
        "\n",
        "    # Create side-by-side comparison (original frame + histogram)\n",
        "    frame_colored = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2BGR)\n",
        "    frame_resized = cv2.resize(frame_colored, (hist_img.shape[1], hist_img.shape[0]))\n",
        "    comparison = np.hstack((frame_resized, hist_img))\n",
        "\n",
        "    # Save the comparison image\n",
        "    comparison_filename = os.path.join(histogram_output_dir, f\"compare_{i:04d}.jpg\")\n",
        "    cv2.imwrite(comparison_filename, comparison)\n",
        "\n",
        "print(f\"Histogram analysis applied to {len(selected_frames)} frames.\")\n",
        "print(f\"Histogram images saved in {histogram_output_dir} directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47Zb89mmKOHD"
      },
      "source": [
        "Histogram Equalization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMEihXTXGO9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7059d161-b334-4921-e91e-98ed3c254c77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Histogram equalization applied to 120 frames.\n",
            "Comparison images saved in equalized_comparison\n"
          ]
        }
      ],
      "source": [
        "# Store equalized frames\n",
        "equalized_frames = []\n",
        "comparison_output_dir = \"equalized_comparison\"\n",
        "os.makedirs(comparison_output_dir, exist_ok=True)\n",
        "\n",
        "for i, gray_frame in enumerate(selected_frames):\n",
        "    equalized = cv2.equalizeHist(gray_frame)\n",
        "    equalized_frames.append(equalized)\n",
        "\n",
        "    # Stack original and equalized side by side for comparison\n",
        "    comparison = np.hstack((gray_frame, equalized))\n",
        "\n",
        "    # Save the side-by-side image\n",
        "    comparison_filename = os.path.join(comparison_output_dir, f\"compare_{i:04d}.jpg\")\n",
        "    cv2.imwrite(comparison_filename, comparison)\n",
        "\n",
        "print(f\"Histogram equalization applied to {len(equalized_frames)} frames.\")\n",
        "print(f\"Comparison images saved in {comparison_output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hTgufSsKSkz"
      },
      "source": [
        "Fourier Transform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQurfTObHeoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97c4c8c-fe61-42de-a271-99ae8663bbc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fourier transform applied to 120 frames.\n",
            "Results saved in fourier_results\n"
          ]
        }
      ],
      "source": [
        "fourier_output_dir = \"fourier_results\"\n",
        "os.makedirs(fourier_output_dir, exist_ok=True)\n",
        "\n",
        "reconstructed_frames = []\n",
        "\n",
        "for i, frame in enumerate(selected_frames):\n",
        "    # 1. Apply 2D Fourier Transform\n",
        "    f = np.fft.fft2(frame)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    # Add 1 to avoid log(0)\n",
        "    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1)\n",
        "\n",
        "    # 2. Apply a Low-Pass Filter (removing high frequencies)\n",
        "    rows, cols = frame.shape\n",
        "    crow, ccol = rows // 2 , cols // 2\n",
        "    mask = np.zeros((rows, cols), np.uint8)\n",
        "    # Tune this for more/less filtering\n",
        "    radius = 50\n",
        "    cv2.circle(mask, (ccol, crow), radius, 1, thickness=-1)\n",
        "\n",
        "    # 3. Filter and Inverse FT\n",
        "    fshift_filtered = fshift * mask\n",
        "    f_ishift = np.fft.ifftshift(fshift_filtered)\n",
        "    img_back = np.fft.ifft2(f_ishift)\n",
        "    img_back = np.abs(img_back).astype(np.uint8)\n",
        "\n",
        "    reconstructed_frames.append(img_back)\n",
        "\n",
        "    # 4. Save visualizations\n",
        "    # Comparison Original | Spectrum | Reconstructed\n",
        "    spectrum_normalized = cv2.normalize(magnitude_spectrum, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "    stacked = np.hstack((frame, spectrum_normalized, img_back))\n",
        "    output_path = os.path.join(fourier_output_dir, f\"fourier_{i:04d}.jpg\")\n",
        "    cv2.imwrite(output_path, stacked)\n",
        "\n",
        "print(f\"Fourier transform applied to {len(selected_frames)} frames.\")\n",
        "print(f\"Results saved in {fourier_output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LxCXQ0QKYXT"
      },
      "source": [
        "Edge Detection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35LMCf5NIk-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d792c0-b84a-4b19-b47e-f0607d2338ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied Canny, Sobel, and Prewitt edge detection on 120 frames.\n",
            "Stacked comparison images saved in 'edges_with_original' folder.\n"
          ]
        }
      ],
      "source": [
        "edge_output_dir = \"edges_with_original\"\n",
        "edge_output_dir2 = \"edges\"\n",
        "os.makedirs(edge_output_dir, exist_ok=True)\n",
        "os.makedirs(edge_output_dir2, exist_ok=True)\n",
        "\n",
        "# Define Prewitt kernels\n",
        "prewitt_kernel_x = np.array([[1, 0, -1],\n",
        "                             [1, 0, -1],\n",
        "                             [1, 0, -1]], dtype=np.float32)\n",
        "\n",
        "prewitt_kernel_y = np.array([[1, 1, 1],\n",
        "                             [0, 0, 0],\n",
        "                             [-1, -1, -1]], dtype=np.float32)\n",
        "\n",
        "for i, frame in enumerate(selected_frames):\n",
        "    # Apply Canny\n",
        "    canny_edges = cv2.Canny(frame, 100, 200)\n",
        "\n",
        "    # Apply Sobel\n",
        "    sobelx = cv2.Sobel(frame, cv2.CV_64F, 1, 0, ksize=3)\n",
        "    sobely = cv2.Sobel(frame, cv2.CV_64F, 0, 1, ksize=3)\n",
        "    sobel_combined = cv2.magnitude(sobelx, sobely)\n",
        "    sobel_combined = np.uint8(np.clip(sobel_combined, 0, 255))\n",
        "\n",
        "    # Apply Prewitt\n",
        "    prewitt_x = cv2.filter2D(frame, -1, prewitt_kernel_x)\n",
        "    prewitt_y = cv2.filter2D(frame, -1, prewitt_kernel_y)\n",
        "    prewitt_combined = cv2.magnitude(prewitt_x.astype(np.float32), prewitt_y.astype(np.float32))\n",
        "    prewitt_combined = np.uint8(np.clip(prewitt_combined, 0, 255))\n",
        "\n",
        "    # Comparison Original | Canny | Sobel | Prewitt\n",
        "    stacked_edges = np.hstack((frame, canny_edges, sobel_combined, prewitt_combined))\n",
        "\n",
        "    #save edges with the orignial image\n",
        "    edge_path = os.path.join(edge_output_dir, f\"edges_with_original_{i:04d}.jpg\")\n",
        "    cv2.imwrite(edge_path, stacked_edges)\n",
        "\n",
        "    #save images with edges only\n",
        "    edge_path2 = os.path.join(edge_output_dir2, f\"edges_{i:04d}.jpg\")\n",
        "    cv2.imwrite(edge_path2, canny_edges)\n",
        "\n",
        "\n",
        "print(f\"Applied Canny, Sobel, and Prewitt edge detection on {len(selected_frames)} frames.\")\n",
        "print(f\"Stacked comparison images saved in '{edge_output_dir}' folder.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYOWInKsKbdT"
      },
      "source": [
        "Kernel Filtering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dq-Xk4GJCSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c76802b-ddbf-4231-d73e-3a2cac50eed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Gaussian + Bilateral filters applied to 120 frames.\n",
            "Results saved in 'kernel_filtered' folder.\n"
          ]
        }
      ],
      "source": [
        "kernel_filter_output_dir = \"kernel_filtered\"\n",
        "os.makedirs(kernel_filter_output_dir, exist_ok=True)\n",
        "\n",
        "for i, frame in enumerate(selected_frames):\n",
        "    # 1. Apply Gaussian Filter first (noise reduction)\n",
        "    gaussian_filtered = cv2.GaussianBlur(frame, (5, 5), 0)\n",
        "\n",
        "    # 2. Apply Bilateral Filter on the Gaussian-filtered result (edge-preserving smoothing)\n",
        "    combined_filtered = cv2.bilateralFilter(gaussian_filtered, 9, 75, 75)\n",
        "\n",
        "    # 3. Compare: Original | Gaussian Only | Combined (Gaussian + Bilateral)\n",
        "    stacked_filtered = np.hstack((frame, gaussian_filtered, combined_filtered))\n",
        "\n",
        "    # Save the comparison images\n",
        "    filter_output_path = os.path.join(kernel_filter_output_dir, f\"filtered_{i:04d}.jpg\")\n",
        "    cv2.imwrite(filter_output_path, stacked_filtered)\n",
        "\n",
        "print(f\"Combined Gaussian + Bilateral filters applied to {len(selected_frames)} frames.\")\n",
        "print(f\"Results saved in '{kernel_filter_output_dir}' folder.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmVjimsbOQWk"
      },
      "source": [
        "# **BONUS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_D5B45g9qCy"
      },
      "source": [
        "Highlight possible lane markings or object boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hDfN-yuOnAf"
      },
      "source": [
        "Use contour detection for precise object segmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2HrrmlXKv0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b58a62-6443-44ae-b7a9-b66218a6977e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lane markings and object boundaries highlighted and saved in 'lane_and_object_boundaries' folder.\n"
          ]
        }
      ],
      "source": [
        "# Directory to save output frames with lane markings and object boundaries\n",
        "output_dir = \"lane_and_object_boundaries\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for i, frame in enumerate(selected_frames):\n",
        "    # Apply Canny Edge Detection (if not done already)\n",
        "    edges = cv2.Canny(frame, 100, 200)\n",
        "\n",
        "    # Apply Hough Transform to detect lanes (lines)\n",
        "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=50, minLineLength=100, maxLineGap=10)\n",
        "    # Make a copy of the original frame to highlight lanes\n",
        "    lane_markings = frame.copy()\n",
        "\n",
        "    # Draw the lines (lane markings) in green\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            x1, y1, x2, y2 = line[0]\n",
        "            cv2.line(lane_markings, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green lines\n",
        "\n",
        "    # Apply Contour Detection for object boundaries\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "     # Make another copy of the original frame for contours\n",
        "    contour_output = frame.copy()\n",
        "\n",
        "    # Draw the contours (object boundaries) in red\n",
        "    for contour in contours:\n",
        "      # Filter out small contours\n",
        "        if cv2.contourArea(contour) > 100:\n",
        "            cv2.drawContours(contour_output, [contour], -1, (0, 0, 255), 2)  # Red contours\n",
        "\n",
        "    # Stack the images for visual comparison: Original | Lane Markings | Object Boundaries\n",
        "    stacked_output = np.hstack((frame, lane_markings, contour_output))\n",
        "\n",
        "    # Save the output frame\n",
        "    output_path = os.path.join(output_dir, f\"frame_{i:04d}.jpg\")\n",
        "    cv2.imwrite(output_path, stacked_output)\n",
        "\n",
        "print(f\"Lane markings and object boundaries highlighted and saved in '{output_dir}' folder.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oiOieM2OiDz"
      },
      "source": [
        "Incorporate Hough Transform HT for robust lane detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0xxWFvSOX_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ff0bc2-4c48-4397-fdab-8b4bb904e93b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robust lane detection applied to 120 frames.\n",
            "Results saved in 'lane_detection_ht' folder.\n"
          ]
        }
      ],
      "source": [
        "# Robust Lane Detection with Hough Transform\n",
        "lane_output_dir = \"lane_detection_ht\"\n",
        "os.makedirs(lane_output_dir, exist_ok=True)\n",
        "\n",
        "for i, frame in enumerate(selected_frames):\n",
        "    # 1. Preprocessing\n",
        "    blurred = cv2.GaussianBlur(frame, (5, 5), 0)\n",
        "    edges = cv2.Canny(blurred, 50, 150)\n",
        "\n",
        "    # 2. Create ROI mask (focus on lower half for lanes)\n",
        "    height, width = edges.shape\n",
        "    mask = np.zeros_like(edges)\n",
        "    vertices = np.array([[\n",
        "        (width * 0.1, height),  # Bottom-left\n",
        "        (width * 0.45, height * 0.6),  # Top-left\n",
        "        (width * 0.55, height * 0.6),  # Top-right\n",
        "        (width * 0.9, height)  # Bottom-right\n",
        "    ]], dtype=np.int32)\n",
        "    cv2.fillPoly(mask, vertices, 255)\n",
        "    masked_edges = cv2.bitwise_and(edges, mask)\n",
        "\n",
        "    # 3. Hough Transform for line detection\n",
        "    lines = cv2.HoughLinesP(masked_edges,\n",
        "                           rho=1,\n",
        "                           theta=np.pi/180,\n",
        "                           threshold=30,\n",
        "                           minLineLength=40,\n",
        "                           maxLineGap=25)\n",
        "\n",
        "    # 4. Process and classify lines\n",
        "    left_lines = []\n",
        "    right_lines = []\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            x1, y1, x2, y2 = line[0]\n",
        "\n",
        "            # Calculate line parameters\n",
        "            if x2 - x1 != 0:  # Avoid division by zero\n",
        "                slope = (y2 - y1) / (x2 - x1)\n",
        "                intercept = y1 - slope * x1\n",
        "\n",
        "                # Classify as left or right lane\n",
        "                if slope < -0.5:  # Negative slope = left lane\n",
        "                    left_lines.append((slope, intercept))\n",
        "                elif slope > 0.5:  # Positive slope = right lane\n",
        "                    right_lines.append((slope, intercept))\n",
        "\n",
        "    # 5. Average and extrapolate lanes\n",
        "    def average_lines(lines, y_min, y_max):\n",
        "        if not lines:\n",
        "            return None\n",
        "\n",
        "        slopes, intercepts = zip(*lines)\n",
        "        avg_slope = np.mean(slopes)\n",
        "        avg_intercept = np.mean(intercepts)\n",
        "\n",
        "        # Calculate x-coordinates\n",
        "        x1 = int((y_min - avg_intercept) / avg_slope)\n",
        "        x2 = int((y_max - avg_intercept) / avg_slope)\n",
        "\n",
        "        return ((x1, y_min, x2, y_max))\n",
        "\n",
        "    y_min = int(height * 0.6)\n",
        "    y_max = height\n",
        "\n",
        "    left_lane = average_lines(left_lines, y_min, y_max)\n",
        "    right_lane = average_lines(right_lines, y_min, y_max)\n",
        "\n",
        "    # 6. Visualization\n",
        "    lane_img = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Draw ROI area\n",
        "    cv2.polylines(lane_img, [vertices], True, (0, 255, 255), 1)\n",
        "\n",
        "    # Draw raw edges\n",
        "    lane_img[masked_edges > 0] = [0, 0, 255]  # Red edges\n",
        "\n",
        "    # Draw detected lanes\n",
        "    if left_lane:\n",
        "        cv2.line(lane_img, (left_lane[0], left_lane[1]),\n",
        "                         (left_lane[2], left_lane[3]), (0, 255, 0), 3)\n",
        "    if right_lane:\n",
        "        cv2.line(lane_img, (right_lane[0], right_lane[1]),\n",
        "                         (right_lane[2], right_lane[3]), (0, 255, 0), 3)\n",
        "\n",
        "    # Draw lane area (if both lanes detected)\n",
        "    if left_lane and right_lane:\n",
        "        lane_area = np.array([\n",
        "            [left_lane[0], left_lane[1]],\n",
        "            [left_lane[2], left_lane[3]],\n",
        "            [right_lane[2], right_lane[3]],\n",
        "            [right_lane[0], right_lane[1]]\n",
        "        ])\n",
        "        cv2.fillPoly(lane_img, [lane_area], (0, 100, 0))\n",
        "\n",
        "    # Save output\n",
        "    output_path = os.path.join(lane_output_dir, f\"lane_{i:04d}.jpg\")\n",
        "    cv2.imwrite(output_path, lane_img)\n",
        "\n",
        "print(f\"Robust lane detection applied to {len(selected_frames)} frames.\")\n",
        "print(f\"Results saved in '{lane_output_dir}' folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How These Techniques Could Be Used in an Actual Autonomous Vehicle System?"
      ],
      "metadata": {
        "id": "GYOBNBdspv15"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm3JaQkCLyJO"
      },
      "source": [
        "1)Lane Detection (Hough Transform):\n",
        "The Hough Transform identifies lane markings, helping the vehicle stay centered in its lane and aiding in lane-keeping assistance.\n",
        "\n",
        "2)Object Boundary Detection (Contours):\n",
        "Detecting object boundaries (e.g., other vehicles, pedestrians) is crucial for collision avoidance, allowing the vehicle to take appropriate actions like slowing down or steering.\n",
        "\n",
        "3)Edge Preservation and Noise Reduction:\n",
        "Techniques like bilateral filtering reduce noise while preserving edges, ensuring clear lane markings and objects in low-contrast conditions (e.g., rain or night).\n",
        "\n",
        "4)Region of Interest (ROI):\n",
        "Focusing on key areas (like lane detection) optimizes processing and improves accuracy, especially in the central region where the vehicle is focused.\n",
        "\n",
        "5)Real-Time Processing:\n",
        "Efficient edge detection and filtering enable the vehicle to quickly analyze its environment and make real-time decisions for lane-keeping and obstacle avoidance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making the videos for each technique**"
      ],
      "metadata": {
        "id": "PKdjcyguAVxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQyMBikjfS2m",
        "outputId": "dd97ceeb-81da-4e5a-ee62-27ff4a329bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created: histogram_comparison.mp4\n",
            "Created: histogram_equalization.mp4\n",
            "Created: fourier_transform.mp4\n",
            "Created: edge_detection.mp4\n",
            "Created: kernel_filtering.mp4\n",
            "Created: lane_object_detection.mp4\n",
            "Created: lane_detection_hough.mp4\n",
            "Created: edges_with_original.mp4\n",
            "\n",
            "Creating combined processing video...\n",
            "Combined video created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create Videos from Processed Frames\n",
        "# Define video parameters\n",
        "video_fps = 20  # Frames per second\n",
        "video_size = (640, 480)  # Unified size for all videos\n",
        "\n",
        "# Create videos for each processing step\n",
        "processing_steps = [\n",
        "    (\"histogram_analysis\", \"histogram_comparison.mp4\"),\n",
        "    (\"equalized_comparison\", \"histogram_equalization.mp4\"),\n",
        "    (\"fourier_results\", \"fourier_transform.mp4\"),\n",
        "    (\"edges\", \"edge_detection.mp4\"),\n",
        "    (\"kernel_filtered\", \"kernel_filtering.mp4\"),\n",
        "    (\"lane_and_object_boundaries\", \"lane_object_detection.mp4\"),\n",
        "    (\"lane_detection_ht\", \"lane_detection_hough.mp4\"),\n",
        "    (\"edges_with_original\",\"edges_with_original.mp4\")\n",
        "    #,\n",
        "   # (\"object_segmentation\", \"object_segmentation.mp4\")\n",
        "]\n",
        "\n",
        "for input_dir, output_video in processing_steps:\n",
        "    image_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir)\n",
        "                        if f.endswith(('.jpg', '.png'))])\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    # Create video writer with unified size\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    video_writer = cv2.VideoWriter(output_video, fourcc, video_fps, video_size)\n",
        "\n",
        "    for image_file in image_files:\n",
        "        img = cv2.imread(image_file)\n",
        "        if img is not None:\n",
        "            img_resized = cv2.resize(img, video_size)\n",
        "            video_writer.write(img_resized)\n",
        "\n",
        "    video_writer.release()\n",
        "    print(f\"Created: {output_video}\")\n",
        "\n",
        "# Create combined video with proper sizing\n",
        "print(\"\\nCreating combined processing video...\")\n",
        "\n",
        "# Define combined video parameters\n",
        "panel_width = 320  # Each panel width\n",
        "panel_height = 240  # Each panel height\n",
        "combined_size = (panel_width * 4, panel_height * 2)  # 4x2 grid (1280x480)\n",
        "\n",
        "processing_steps = [step[0] for step in processing_steps]\n",
        "all_images = {step: sorted(os.listdir(step)) for step in processing_steps}\n",
        "min_frames = min(len(images) for images in all_images.values())\n",
        "\n",
        "# Create blank image template for missing frames\n",
        "blank_panel = np.zeros((panel_height, panel_width, 3), dtype=np.uint8)\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "combined_writer = cv2.VideoWriter(\"combined_processing.mp4\", fourcc, video_fps, combined_size)\n",
        "\n",
        "for i in range(min_frames):\n",
        "    rows = []\n",
        "    for row in range(2):\n",
        "        current_row = []\n",
        "        for col in range(4):\n",
        "            step_idx = row * 4 + col\n",
        "            if step_idx >= len(processing_steps):\n",
        "                # Add blank panel if no more steps\n",
        "                current_row.append(blank_panel)\n",
        "                continue\n",
        "\n",
        "            step = processing_steps[step_idx]\n",
        "            img_path = os.path.join(step, all_images[step][i])\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is not None:\n",
        "                img_resized = cv2.resize(img, (panel_width, panel_height))\n",
        "                current_row.append(img_resized)\n",
        "            else:\n",
        "                # Add blank panel if image loading fails\n",
        "                current_row.append(blank_panel)\n",
        "\n",
        "        # Ensure exactly 4 panels per row\n",
        "        row_stack = np.hstack(current_row)\n",
        "        rows.append(row_stack)\n",
        "\n",
        "    # Combine rows vertically\n",
        "    final_frame = np.vstack(rows)\n",
        "\n",
        "    # Verify final dimensions before writing\n",
        "    if final_frame.shape[1] == combined_size[0] and final_frame.shape[0] == combined_size[1]:\n",
        "        combined_writer.write(final_frame)\n",
        "    else:\n",
        "        print(f\"Skipping frame {i} due to incorrect dimensions: {final_frame.shape}\")\n",
        "\n",
        "combined_writer.release()\n",
        "print(\"Combined video created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_processed_frames(directory):\n",
        "    frames = []\n",
        "    files = sorted([f for f in os.listdir(directory) if f.endswith(('.jpg','.png'))])\n",
        "    for f in files:\n",
        "        img = cv2.imread(os.path.join(directory, f))\n",
        "        if img is not None:\n",
        "            frames.append(img)\n",
        "    return frames\n",
        "\n",
        "# Load the three techniques\n",
        "lane_frames = load_processed_frames(\"lane_detection_ht\")\n",
        "filter_frames = load_processed_frames(\"kernel_filtered\")\n",
        "edge_frames = load_processed_frames(\"edges\")\n",
        "\n",
        "# Video parameters\n",
        "output_fps = 20\n",
        "min_frames = min(len(lane_frames), len(filter_frames), len(edge_frames))\n",
        "composite_frames = []\n",
        "\n",
        "# Standard dimensions for each technique's output\n",
        "tech_width = 640\n",
        "tech_height = 240  # Fixed height for each technique section\n",
        "composite_width = 1280\n",
        "composite_height = tech_height * 3 + 100  # 3 sections + padding\n",
        "\n",
        "for i in range(min_frames):\n",
        "    # Create blank canvas (black background)\n",
        "    composite = np.zeros((composite_height, composite_width, 3), dtype=np.uint8)\n",
        "\n",
        "    # 1. Lane Detection (top section)\n",
        "    lane_img = lane_frames[i]\n",
        "    lane_resized = cv2.resize(lane_img, (tech_width, tech_height))\n",
        "    composite[20:20+tech_height, (composite_width-tech_width)//2:(composite_width-tech_width)//2+tech_width] = lane_resized\n",
        "\n",
        "    # 2. Kernel Filtering (middle section)\n",
        "    filter_full = filter_frames[i]\n",
        "    # Extract just the filtered portion (rightmost panel)\n",
        "    filter_img = filter_full[:, filter_full.shape[1]//3*2:]\n",
        "    filter_resized = cv2.resize(filter_img, (tech_width, tech_height))\n",
        "    y_offset = 20 + tech_height + 20\n",
        "    composite[y_offset:y_offset+tech_height, (composite_width-tech_width)//2:(composite_width-tech_width)//2+tech_width] = filter_resized\n",
        "\n",
        "    # 3. Edge Detection (bottom section)\n",
        "    edge_full = edge_frames[i]\n",
        "    # Extract just the Canny edges portion (second panel)\n",
        "    edge_img = edge_full[:, edge_full.shape[1]//4:edge_full.shape[1]//4*2]\n",
        "    edge_resized = cv2.resize(edge_img, (tech_width, tech_height))\n",
        "    y_offset = y_offset + tech_height + 20\n",
        "    composite[y_offset:y_offset+tech_height, (composite_width-tech_width)//2:(composite_width-tech_width)//2+tech_width] = edge_resized\n",
        "\n",
        "    # Add labels\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.putText(composite, \"Lane Detection\", (50, 15), font, 0.7, (255,255,255), 2)\n",
        "    cv2.putText(composite, \"Kernel Filtering\", (50, 20 + tech_height + 15), font, 0.7, (255,255,255), 2)\n",
        "    cv2.putText(composite, \"Edge Detection (Canny)\", (50, 20 + tech_height*2 + 35), font, 0.7, (255,255,255), 2)\n",
        "\n",
        "    composite_frames.append(cv2.cvtColor(composite, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# Create final video\n",
        "final_clip = ImageSequenceClip(composite_frames, fps=output_fps)\n",
        "final_clip.write_videofile(\n",
        "    \"combined_techniques.mp4\",\n",
        "    codec=\"libx264\",\n",
        "    fps=output_fps,\n",
        "    preset=\"medium\",\n",
        "    ffmpeg_params=[\"-pix_fmt\", \"yuv420p\"],\n",
        "    threads=4,\n",
        "    audio=False\n",
        ")\n",
        "\n",
        "print(\"Final video created successfully as 'combined_techniques.mp4'\")"
      ],
      "metadata": {
        "id": "7Kgv-lzv0a7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae2c8f2-bb70-4e92-9835-851b46c93158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video combined_techniques.mp4.\n",
            "Moviepy - Writing video combined_techniques.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready combined_techniques.mp4\n",
            "Final video created successfully as 'combined_techniques.mp4'\n"
          ]
        }
      ]
    }
  ]
}